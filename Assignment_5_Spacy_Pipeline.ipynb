{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNxOoORrTK9aai4nOM+epcS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vvrgit/NLP-LAB/blob/main/Assignment_5_Spacy_Pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E6qe8-DrpnIw",
        "outputId": "82deacdb-8645-4313-d32a-8ec4c3a2ea2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
            "0  570306133677760513           neutral                        1.0000   \n",
            "1  570301130888122368          positive                        0.3486   \n",
            "2  570301083672813571           neutral                        0.6837   \n",
            "3  570301031407624196          negative                        1.0000   \n",
            "4  570300817074462722          negative                        1.0000   \n",
            "\n",
            "  negativereason  negativereason_confidence         airline  \\\n",
            "0            NaN                        NaN  Virgin America   \n",
            "1            NaN                     0.0000  Virgin America   \n",
            "2            NaN                        NaN  Virgin America   \n",
            "3     Bad Flight                     0.7033  Virgin America   \n",
            "4     Can't Tell                     1.0000  Virgin America   \n",
            "\n",
            "  airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n",
            "0                    NaN     cairdin                 NaN              0   \n",
            "1                    NaN    jnardino                 NaN              0   \n",
            "2                    NaN  yvonnalynn                 NaN              0   \n",
            "3                    NaN    jnardino                 NaN              0   \n",
            "4                    NaN    jnardino                 NaN              0   \n",
            "\n",
            "                                                text tweet_coord  \\\n",
            "0                @VirginAmerica What @dhepburn said.         NaN   \n",
            "1  @VirginAmerica plus you've added commercials t...         NaN   \n",
            "2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n",
            "3  @VirginAmerica it's really aggressive to blast...         NaN   \n",
            "4  @VirginAmerica and it's a really big bad thing...         NaN   \n",
            "\n",
            "               tweet_created tweet_location               user_timezone  \n",
            "0  2015-02-24 11:35:52 -0800            NaN  Eastern Time (US & Canada)  \n",
            "1  2015-02-24 11:15:59 -0800            NaN  Pacific Time (US & Canada)  \n",
            "2  2015-02-24 11:15:48 -0800      Lets Play  Central Time (US & Canada)  \n",
            "3  2015-02-24 11:15:36 -0800            NaN  Pacific Time (US & Canada)  \n",
            "4  2015-02-24 11:14:45 -0800            NaN  Pacific Time (US & Canada)  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "tweets_df = pd.read_csv('/content/Tweets.csv')\n",
        "print(tweets_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tweets = tweets_df['text']\n",
        "print(\"Extracted tweets (first 5):\")\n",
        "for i, tweet in enumerate(tweets.head()):\n",
        "    print(f\"{i+1}. {tweet}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdgLGsaGrwXH",
        "outputId": "bec00b6b-21d4-4031-f91b-7abce93589bd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted tweets (first 5):\n",
            "1. @VirginAmerica What @dhepburn said.\n",
            "2. @VirginAmerica plus you've added commercials to the experience... tacky.\n",
            "3. @VirginAmerica I didn't today... Must mean I need to take another trip!\n",
            "4. @VirginAmerica it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &amp; they have little recourse\n",
            "5. @VirginAmerica and it's a really big bad thing about it\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36e208b9"
      },
      "source": [
        "# Task\n",
        "Preprocess the text in the `tweets` Series by cleaning, tokenizing, removing stopwords, and lemmatizing the tokens, and then join the processed tokens back into a string."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3fb5102"
      },
      "source": [
        "## Clean Text\n",
        "\n",
        "### Subtask:\n",
        "Remove mentions, URLs, special characters, and convert text to lowercase.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "082a0697"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires removing mentions, URLs, special characters, and converting text to lowercase. I will define a function to perform these cleaning steps using regular expressions and then apply it to the 'tweets' Series.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "191080de",
        "outputId": "dbafeeed-dcec-4702-d5b3-3738b0830537"
      },
      "source": [
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "    # Remove mentions\n",
        "    text = re.sub(r'@\\w+', '', text)\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
        "    # Remove special characters and numbers, keep spaces\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    return text.strip()\n",
        "\n",
        "cleaned_tweets = tweets.apply(clean_text)\n",
        "\n",
        "print(\"Original tweets (first 5):\")\n",
        "for i, tweet in enumerate(tweets.head()):\n",
        "    print(f\"{i+1}. {tweet}\")\n",
        "\n",
        "print(\"\\nCleaned tweets (first 5):\")\n",
        "for i, tweet in enumerate(cleaned_tweets.head()):\n",
        "    print(f\"{i+1}. {tweet}\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original tweets (first 5):\n",
            "1. @VirginAmerica What @dhepburn said.\n",
            "2. @VirginAmerica plus you've added commercials to the experience... tacky.\n",
            "3. @VirginAmerica I didn't today... Must mean I need to take another trip!\n",
            "4. @VirginAmerica it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &amp; they have little recourse\n",
            "5. @VirginAmerica and it's a really big bad thing about it\n",
            "\n",
            "Cleaned tweets (first 5):\n",
            "1. what  said\n",
            "2. plus youve added commercials to the experience tacky\n",
            "3. i didnt today must mean i need to take another trip\n",
            "4. its really aggressive to blast obnoxious entertainment in your guests faces amp they have little recourse\n",
            "5. and its a really big bad thing about it\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3f85bff7"
      },
      "source": [
        "## Tokenize Text\n",
        "\n",
        "### Subtask:\n",
        "Tokenize the cleaned text into individual words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "002279b0"
      },
      "source": [
        "**Reasoning**:\n",
        "The next step is to tokenize the cleaned text. I will use NLTK's `word_tokenize` function for this, but first, I need to ensure NLTK and its required data ('punkt') are installed and downloaded.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ff7f92a",
        "outputId": "bb664332-c6f3-4eae-fde6-b6097248d529"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def tokenize_text(text):\n",
        "    return word_tokenize(text)\n",
        "\n",
        "tokenized_tweets = cleaned_tweets.apply(tokenize_text)\n",
        "\n",
        "print(\"Cleaned tweets (first 5):\")\n",
        "for i, tweet in enumerate(cleaned_tweets.head()):\n",
        "    print(f\"{i+1}. {tweet}\")\n",
        "\n",
        "print(\"\\nTokenized tweets (first 5):\")\n",
        "for i, tokens in enumerate(tokenized_tweets.head()):\n",
        "    print(f\"{i+1}. {tokens}\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned tweets (first 5):\n",
            "1. what  said\n",
            "2. plus youve added commercials to the experience tacky\n",
            "3. i didnt today must mean i need to take another trip\n",
            "4. its really aggressive to blast obnoxious entertainment in your guests faces amp they have little recourse\n",
            "5. and its a really big bad thing about it\n",
            "\n",
            "Tokenized tweets (first 5):\n",
            "1. ['what', 'said']\n",
            "2. ['plus', 'youve', 'added', 'commercials', 'to', 'the', 'experience', 'tacky']\n",
            "3. ['i', 'didnt', 'today', 'must', 'mean', 'i', 'need', 'to', 'take', 'another', 'trip']\n",
            "4. ['its', 'really', 'aggressive', 'to', 'blast', 'obnoxious', 'entertainment', 'in', 'your', 'guests', 'faces', 'amp', 'they', 'have', 'little', 'recourse']\n",
            "5. ['and', 'its', 'a', 'really', 'big', 'bad', 'thing', 'about', 'it']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kheql6ms5nvt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2045de21"
      },
      "source": [
        "# Task\n",
        "Remove stopwords from the `tokenized_tweets` Series."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "504d3c5e"
      },
      "source": [
        "## Remove Stopwords\n",
        "\n",
        "### Subtask:\n",
        "Eliminate common words that do not carry significant meaning from the tokenized text.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b43d2935"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to ensure the NLTK 'stopwords' corpus is downloaded and then define and apply a function to remove stopwords from the tokenized text.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "355dc8e7",
        "outputId": "43f8ba8d-1f72-4fd7-bbe9-436ed73edd91"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Create a set of English stopwords for efficient lookup\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def remove_stopwords(tokens):\n",
        "    return [word for word in tokens if word not in stop_words]\n",
        "\n",
        "filtered_tweets = tokenized_tweets.apply(remove_stopwords)\n",
        "\n",
        "print(\"Tokenized tweets (first 5):\")\n",
        "for i, tokens in enumerate(tokenized_tweets.head()):\n",
        "    print(f\"{i+1}. {tokens}\")\n",
        "\n",
        "print(\"\\nFiltered tweets (first 5) (stopwords removed):\")\n",
        "for i, tokens in enumerate(filtered_tweets.head()):\n",
        "    print(f\"{i+1}. {tokens}\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized tweets (first 5):\n",
            "1. ['what', 'said']\n",
            "2. ['plus', 'youve', 'added', 'commercials', 'to', 'the', 'experience', 'tacky']\n",
            "3. ['i', 'didnt', 'today', 'must', 'mean', 'i', 'need', 'to', 'take', 'another', 'trip']\n",
            "4. ['its', 'really', 'aggressive', 'to', 'blast', 'obnoxious', 'entertainment', 'in', 'your', 'guests', 'faces', 'amp', 'they', 'have', 'little', 'recourse']\n",
            "5. ['and', 'its', 'a', 'really', 'big', 'bad', 'thing', 'about', 'it']\n",
            "\n",
            "Filtered tweets (first 5) (stopwords removed):\n",
            "1. ['said']\n",
            "2. ['plus', 'youve', 'added', 'commercials', 'experience', 'tacky']\n",
            "3. ['didnt', 'today', 'must', 'mean', 'need', 'take', 'another', 'trip']\n",
            "4. ['really', 'aggressive', 'blast', 'obnoxious', 'entertainment', 'guests', 'faces', 'amp', 'little', 'recourse']\n",
            "5. ['really', 'big', 'bad', 'thing']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tKvYfq6N6rMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fca40ec0"
      },
      "source": [
        "# Task\n",
        "Lemmatize the tokens in the `filtered_tweets` Series, download the necessary NLTK corpora (`wordnet` and `omw-1.4`), then join the lemmatized tokens back into a single string for each tweet, and display the first few lemmatized tweets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ac0fcca1"
      },
      "source": [
        "## Lemmatize Tokens\n",
        "\n",
        "### Subtask:\n",
        "Reduce tokens to their base or dictionary form using NLTK's WordNetLemmatizer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ca621737"
      },
      "source": [
        "#### Reasoning:\n",
        "To further normalize the text and group together words with similar meanings (e.g., 'run', 'runs', 'running'), I will lemmatize the filtered tokens. This involves using NLTK's WordNetLemmatizer, which requires downloading the 'wordnet' and 'omw-1.4' corpora. I will then define a function to apply the lemmatizer to each token and map this function over the `filtered_tweets` Series."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9e6834a"
      },
      "source": [
        "**Reasoning**:\n",
        "As per the instructions, I need to download the required NLTK corpora ('wordnet' and 'omw-1.4'), import `WordNetLemmatizer`, initialize it, define a function to lemmatize tokens, apply this function to the `filtered_tweets` Series, and then display the first five lemmatized tweet lists.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45d5e5e2",
        "outputId": "f8d592aa-2a53-46b3-82bf-b54d0f9613e4"
      },
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Initialize WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def lemmatize_tokens(tokens):\n",
        "    return [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "lemmatized_tweets = filtered_tweets.apply(lemmatize_tokens)\n",
        "\n",
        "print(\"Filtered tweets (first 5):\")\n",
        "for i, tokens in enumerate(filtered_tweets.head()):\n",
        "    print(f\"{i+1}. {tokens}\")\n",
        "\n",
        "print(\"\\nLemmatized tweets (first 5):\")\n",
        "for i, tokens in enumerate(lemmatized_tweets.head()):\n",
        "    print(f\"{i+1}. {tokens}\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered tweets (first 5):\n",
            "1. ['said']\n",
            "2. ['plus', 'youve', 'added', 'commercials', 'experience', 'tacky']\n",
            "3. ['didnt', 'today', 'must', 'mean', 'need', 'take', 'another', 'trip']\n",
            "4. ['really', 'aggressive', 'blast', 'obnoxious', 'entertainment', 'guests', 'faces', 'amp', 'little', 'recourse']\n",
            "5. ['really', 'big', 'bad', 'thing']\n",
            "\n",
            "Lemmatized tweets (first 5):\n",
            "1. ['said']\n",
            "2. ['plus', 'youve', 'added', 'commercial', 'experience', 'tacky']\n",
            "3. ['didnt', 'today', 'must', 'mean', 'need', 'take', 'another', 'trip']\n",
            "4. ['really', 'aggressive', 'blast', 'obnoxious', 'entertainment', 'guest', 'face', 'amp', 'little', 'recourse']\n",
            "5. ['really', 'big', 'bad', 'thing']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6d23035c",
        "outputId": "b68db790-8ae8-44c1-952d-a0418e473b37"
      },
      "source": [
        "def join_tokens(tokens):\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "preprocessed_tweets = lemmatized_tweets.apply(join_tokens)\n",
        "\n",
        "print(\"Preprocessed tweets (first 5):\")\n",
        "for i, tweet in enumerate(preprocessed_tweets.head()):\n",
        "    print(f\"{i+1}. {tweet}\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessed tweets (first 5):\n",
            "1. said\n",
            "2. plus youve added commercial experience tacky\n",
            "3. didnt today must mean need take another trip\n",
            "4. really aggressive blast obnoxious entertainment guest face amp little recourse\n",
            "5. really big bad thing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PoS Tagging on Lemmatized Tweets**"
      ],
      "metadata": {
        "id": "LMCQDFxf9SMF"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0491d25d",
        "outputId": "f398671f-17d1-4e56-d372-494f870d276f"
      },
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "from nltk import pos_tag\n",
        "\n",
        "def tag_tokens(tokens):\n",
        "    return pos_tag(tokens)\n",
        "\n",
        "pos_tagged_tweets = lemmatized_tweets.apply(tag_tokens)\n",
        "\n",
        "print(\"Lemmatized tweets (first 5):\")\n",
        "for i, tokens in enumerate(lemmatized_tweets.head()):\n",
        "    print(f\"{i+1}. {tokens}\")\n",
        "\n",
        "print(\"\\nPOS Tagged tweets (first 5):\")\n",
        "for i, tagged_tokens in enumerate(pos_tagged_tweets.head()):\n",
        "    print(f\"{i+1}. {tagged_tokens}\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatized tweets (first 5):\n",
            "1. ['said']\n",
            "2. ['plus', 'youve', 'added', 'commercial', 'experience', 'tacky']\n",
            "3. ['didnt', 'today', 'must', 'mean', 'need', 'take', 'another', 'trip']\n",
            "4. ['really', 'aggressive', 'blast', 'obnoxious', 'entertainment', 'guest', 'face', 'amp', 'little', 'recourse']\n",
            "5. ['really', 'big', 'bad', 'thing']\n",
            "\n",
            "POS Tagged tweets (first 5):\n",
            "1. [('said', 'VBD')]\n",
            "2. [('plus', 'CC'), ('youve', 'NN'), ('added', 'JJ'), ('commercial', 'JJ'), ('experience', 'NN'), ('tacky', 'NN')]\n",
            "3. [('didnt', 'NN'), ('today', 'NN'), ('must', 'MD'), ('mean', 'VB'), ('need', 'MD'), ('take', 'VB'), ('another', 'DT'), ('trip', 'NN')]\n",
            "4. [('really', 'RB'), ('aggressive', 'JJ'), ('blast', 'NN'), ('obnoxious', 'JJ'), ('entertainment', 'NN'), ('guest', 'JJS'), ('face', 'NN'), ('amp', 'NN'), ('little', 'JJ'), ('recourse', 'NN')]\n",
            "5. [('really', 'RB'), ('big', 'JJ'), ('bad', 'JJ'), ('thing', 'NN')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NrSRp2lA9FYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0425bcc"
      },
      "source": [
        "# Task\n",
        "Define a Python function to detect hashtags (words starting with '#') from a list of tokens, apply this function to the `lemmatized_tweets` Series, and display the first few detected hashtags."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3b7a29cd"
      },
      "source": [
        "## Define Hashtag Detector Function\n",
        "\n",
        "### Subtask:\n",
        "Create a Python function that takes a list of tokens and returns a list of identified hashtags (words starting with '#').\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "211b3371"
      },
      "source": [
        "**Reasoning**:\n",
        "I will define the `detect_hashtags` function as specified, which iterates through a list of tokens, identifies those starting with '#', and returns them. I will then demonstrate its usage with a sample tweet from `lemmatized_tweets`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1592f628",
        "outputId": "e1e743fd-d13f-4603-b059-5078dcab37df"
      },
      "source": [
        "def detect_hashtags(tokens):\n",
        "    hashtags = []\n",
        "    for token in tokens:\n",
        "        if token.startswith('#'):\n",
        "            hashtags.append(token)\n",
        "    return hashtags\n",
        "\n",
        "# Example usage with a sample tweet from lemmatized_tweets\n",
        "# For demonstration, let's manually add some hashtags to a lemmatized tweet sample\n",
        "sample_tweet_tokens = lemmatized_tweets.iloc[3] + ['#aggressive', '#obnoxious']\n",
        "\n",
        "detected_hashtags = detect_hashtags(sample_tweet_tokens)\n",
        "\n",
        "print(f\"Original tokens (sample with added hashtags): {sample_tweet_tokens}\")\n",
        "print(f\"Detected hashtags: {detected_hashtags}\")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original tokens (sample with added hashtags): ['really', 'aggressive', 'blast', 'obnoxious', 'entertainment', 'guest', 'face', 'amp', 'little', 'recourse', '#aggressive', '#obnoxious']\n",
            "Detected hashtags: ['#aggressive', '#obnoxious']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fee0c592"
      },
      "source": [
        "**Reasoning**:\n",
        "The `detect_hashtags` function has been defined and tested with a sample. Now, I will apply this function to the entire `lemmatized_tweets` Series to extract hashtags from all tweets and display the first few results.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6a7b4f48",
        "outputId": "86ba381f-c8a3-4926-e1fa-083c33e059f2"
      },
      "source": [
        "all_detected_hashtags = lemmatized_tweets.apply(detect_hashtags)\n",
        "\n",
        "print(\"Detected hashtags for first 5 tweets:\")\n",
        "for i, hashtags in enumerate(all_detected_hashtags.head()):\n",
        "    print(f\"{i+1}. {hashtags}\")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected hashtags for first 5 tweets:\n",
            "1. []\n",
            "2. []\n",
            "3. []\n",
            "4. []\n",
            "5. []\n"
          ]
        }
      ]
    }
  ]
}