{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMOhKFdEb6Qj6G+We45a8rc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vvrgit/NLP-LAB/blob/main/Assignment5_3_Spacy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset: https://www.kaggle.com/datasets/spsayakpaul/arxiv-paper-abstracts?resource=download"
      ],
      "metadata": {
        "id": "_t83fsW01R-p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load Dataset**"
      ],
      "metadata": {
        "id": "nSVEXXvt1uyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('arxiv_data.csv', engine='python', nrows=1000)\n",
        "display(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "oA8_JkqZ1SWw",
        "outputId": "faace86a-ecdc-4fec-8596-58a0d0a4d638"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                              titles  \\\n",
              "0  Survey on Semantic Stereo Matching / Semantic ...   \n",
              "1  FUTURE-AI: Guiding Principles and Consensus Re...   \n",
              "2  Enforcing Mutual Consistency of Hard Regions f...   \n",
              "3  Parameter Decoupling Strategy for Semi-supervi...   \n",
              "4  Background-Foreground Segmentation for Interio...   \n",
              "\n",
              "                                           summaries  \\\n",
              "0  Stereo matching is one of the widely used tech...   \n",
              "1  The recent advancements in artificial intellig...   \n",
              "2  In this paper, we proposed a novel mutual cons...   \n",
              "3  Consistency training has proven to be an advan...   \n",
              "4  To ensure safety in automated driving, the cor...   \n",
              "\n",
              "                         terms  \n",
              "0           ['cs.CV', 'cs.LG']  \n",
              "1  ['cs.CV', 'cs.AI', 'cs.LG']  \n",
              "2           ['cs.CV', 'cs.AI']  \n",
              "3                    ['cs.CV']  \n",
              "4           ['cs.CV', 'cs.LG']  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4e021f4f-2106-480a-bd05-38c61a06660d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>titles</th>\n",
              "      <th>summaries</th>\n",
              "      <th>terms</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Survey on Semantic Stereo Matching / Semantic ...</td>\n",
              "      <td>Stereo matching is one of the widely used tech...</td>\n",
              "      <td>['cs.CV', 'cs.LG']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>FUTURE-AI: Guiding Principles and Consensus Re...</td>\n",
              "      <td>The recent advancements in artificial intellig...</td>\n",
              "      <td>['cs.CV', 'cs.AI', 'cs.LG']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Enforcing Mutual Consistency of Hard Regions f...</td>\n",
              "      <td>In this paper, we proposed a novel mutual cons...</td>\n",
              "      <td>['cs.CV', 'cs.AI']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Parameter Decoupling Strategy for Semi-supervi...</td>\n",
              "      <td>Consistency training has proven to be an advan...</td>\n",
              "      <td>['cs.CV']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Background-Foreground Segmentation for Interio...</td>\n",
              "      <td>To ensure safety in automated driving, the cor...</td>\n",
              "      <td>['cs.CV', 'cs.LG']</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4e021f4f-2106-480a-bd05-38c61a06660d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-4e021f4f-2106-480a-bd05-38c61a06660d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-4e021f4f-2106-480a-bd05-38c61a06660d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"titles\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"FUTURE-AI: Guiding Principles and Consensus Recommendations for Trustworthy Artificial Intelligence in Future Medical Imaging\",\n          \"Background-Foreground Segmentation for Interior Sensing in Automotive Industry\",\n          \"Enforcing Mutual Consistency of Hard Regions for Semi-supervised Medical Image Segmentation\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"summaries\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"The recent advancements in artificial intelligence (AI) combined with the\\nextensive amount of data generated by today's clinical systems, has led to the\\ndevelopment of imaging AI solutions across the whole value chain of medical\\nimaging, including image reconstruction, medical image segmentation,\\nimage-based diagnosis and treatment planning. Notwithstanding the successes and\\nfuture potential of AI in medical imaging, many stakeholders are concerned of\\nthe potential risks and ethical implications of imaging AI solutions, which are\\nperceived as complex, opaque, and difficult to comprehend, utilise, and trust\\nin critical clinical applications. Despite these concerns and risks, there are\\ncurrently no concrete guidelines and best practices for guiding future AI\\ndevelopments in medical imaging towards increased trust, safety and adoption.\\nTo bridge this gap, this paper introduces a careful selection of guiding\\nprinciples drawn from the accumulated experiences, consensus, and best\\npractices from five large European projects on AI in Health Imaging. These\\nguiding principles are named FUTURE-AI and its building blocks consist of (i)\\nFairness, (ii) Universality, (iii) Traceability, (iv) Usability, (v) Robustness\\nand (vi) Explainability. In a step-by-step approach, these guidelines are\\nfurther translated into a framework of concrete recommendations for specifying,\\ndeveloping, evaluating, and deploying technically, clinically and ethically\\ntrustworthy AI solutions into clinical practice.\",\n          \"To ensure safety in automated driving, the correct perception of the\\nsituation inside the car is as important as its environment. Thus, seat\\noccupancy detection and classification of detected instances play an important\\nrole in interior sensing. By the knowledge of the seat occupancy status, it is\\npossible to, e.g., automate the airbag deployment control. Furthermore, the\\npresence of a driver, which is necessary for partially automated driving cars\\nat the automation levels two to four can be verified. In this work, we compare\\ndifferent statistical methods from the field of image segmentation to approach\\nthe problem of background-foreground segmentation in camera based interior\\nsensing. In the recent years, several methods based on different techniques\\nhave been developed and applied to images or videos from different\\napplications. The peculiarity of the given scenarios of interior sensing is,\\nthat the foreground instances and the background both contain static as well as\\ndynamic elements. In data considered in this work, even the camera position is\\nnot completely fixed. We review and benchmark three different methods ranging,\\ni.e., Gaussian Mixture Models (GMM), Morphological Snakes and a deep neural\\nnetwork, namely a Mask R-CNN. In particular, the limitations of the classical\\nmethods, GMM and Morphological Snakes, for interior sensing are shown.\\nFurthermore, it turns, that it is possible to overcome these limitations by\\ndeep learning, e.g.\\\\ using a Mask R-CNN. Although only a small amount of ground\\ntruth data was available for training, we enabled the Mask R-CNN to produce\\nhigh quality background-foreground masks via transfer learning. Moreover, we\\ndemonstrate that certain augmentation as well as pre- and post-processing\\nmethods further enhance the performance of the investigated methods.\",\n          \"In this paper, we proposed a novel mutual consistency network (MC-Net+) to\\neffectively exploit the unlabeled hard regions for semi-supervised medical\\nimage segmentation. The MC-Net+ model is motivated by the observation that deep\\nmodels trained with limited annotations are prone to output highly uncertain\\nand easily mis-classified predictions in the ambiguous regions (e.g. adhesive\\nedges or thin branches) for the image segmentation task. Leveraging these\\nregion-level challenging samples can make the semi-supervised segmentation\\nmodel training more effective. Therefore, our proposed MC-Net+ model consists\\nof two new designs. First, the model contains one shared encoder and multiple\\nsightly different decoders (i.e. using different up-sampling strategies). The\\nstatistical discrepancy of multiple decoders' outputs is computed to denote the\\nmodel's uncertainty, which indicates the unlabeled hard regions. Second, a new\\nmutual consistency constraint is enforced between one decoder's probability\\noutput and other decoders' soft pseudo labels. In this way, we minimize the\\nmodel's uncertainty during training and force the model to generate invariant\\nand low-entropy results in such challenging areas of unlabeled data, in order\\nto learn a generalized feature representation. We compared the segmentation\\nresults of the MC-Net+ with five state-of-the-art semi-supervised approaches on\\nthree public medical datasets. Extension experiments with two common\\nsemi-supervised settings demonstrate the superior performance of our model over\\nother existing methods, which sets a new state of the art for semi-supervised\\nmedical image segmentation.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"terms\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"['cs.CV', 'cs.AI', 'cs.LG']\",\n          \"['cs.CV']\",\n          \"['cs.CV', 'cs.LG']\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Text Pre-Processing Using Regular Expression**"
      ],
      "metadata": {
        "id": "xhTKV2CH1xZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "    # Remove HTML tags\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "    # Remove mentions\n",
        "    text = re.sub(r'@\\w+', '', text)\n",
        "    # Remove hashtags\n",
        "    text = re.sub(r'#\\w+', '', text)\n",
        "\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "\n",
        "    # Remove emojis (a basic approach for common emoji ranges)\n",
        "    emoji_pattern = re.compile(\n",
        "        \"[\"\n",
        "        \"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        \"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        \"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        \"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "        \"\\U00002702-\\U000027B0\"\n",
        "        \"\\U000024C2-\\U0001F251\"\n",
        "        \"]+\", flags=re.UNICODE\n",
        "    )\n",
        "    text = emoji_pattern.sub(r'', text)\n",
        "\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)  # Remove special characters\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
        "    return text"
      ],
      "metadata": {
        "id": "YLMCA85b1ogp"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['processed_summaries'] = df['summaries'].apply(preprocess_text)\n",
        "print(df[['summaries', 'processed_summaries']].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K8e5PUEM1rR4",
        "outputId": "dc052f73-43ec-46b6-8e63-8da09b357fdc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                           summaries  \\\n",
            "0  Stereo matching is one of the widely used tech...   \n",
            "1  The recent advancements in artificial intellig...   \n",
            "2  In this paper, we proposed a novel mutual cons...   \n",
            "3  Consistency training has proven to be an advan...   \n",
            "4  To ensure safety in automated driving, the cor...   \n",
            "\n",
            "                                 processed_summaries  \n",
            "0  stereo matching is one of the widely used tech...  \n",
            "1  the recent advancements in artificial intellig...  \n",
            "2  in this paper we proposed a novel mutual consi...  \n",
            "3  consistency training has proven to be an advan...  \n",
            "4  to ensure safety in automated driving the corr...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Word Tokenization  Using Spacy**"
      ],
      "metadata": {
        "id": "pEI5F2Y212zA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load the English language model. You might need to run `python -m spacy download en_core_web_sm` once if it's not already installed.\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "def tokenize_text_spacy(text):\n",
        "    doc = nlp(text)\n",
        "    return [token.text for token in doc]\n",
        "\n",
        "df['tokenized_summaries'] = df['processed_summaries'].apply(tokenize_text_spacy)\n",
        "\n",
        "print(df[['processed_summaries', 'tokenized_summaries']].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-DVo7sG1tFI",
        "outputId": "de892aee-fc7b-4f91-d04b-594dbe740fc2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                 processed_summaries  \\\n",
            "0  stereo matching is one of the widely used tech...   \n",
            "1  the recent advancements in artificial intellig...   \n",
            "2  in this paper we proposed a novel mutual consi...   \n",
            "3  consistency training has proven to be an advan...   \n",
            "4  to ensure safety in automated driving the corr...   \n",
            "\n",
            "                                 tokenized_summaries  \n",
            "0  [stereo, matching, is, one, of, the, widely, u...  \n",
            "1  [the, recent, advancements, in, artificial, in...  \n",
            "2  [in, this, paper, we, proposed, a, novel, mutu...  \n",
            "3  [consistency, training, has, proven, to, be, a...  \n",
            "4  [to, ensure, safety, in, automated, driving, t...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **stopword removel using spacy**"
      ],
      "metadata": {
        "id": "4pkGAleM2o9p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stopwords_spacy(tokens):\n",
        "    return [token.text for token in nlp(' '.join(tokens)) if not token.is_stop]\n",
        "\n",
        "df['summaries_no_stopwords'] = df['tokenized_summaries'].apply(remove_stopwords_spacy)\n",
        "\n",
        "print(df[['tokenized_summaries', 'summaries_no_stopwords']].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lKSIkRGm2sFp",
        "outputId": "767efa2f-f4c0-4360-e8d9-c519b108ed27"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                 tokenized_summaries  \\\n",
            "0  [stereo, matching, is, one, of, the, widely, u...   \n",
            "1  [the, recent, advancements, in, artificial, in...   \n",
            "2  [in, this, paper, we, proposed, a, novel, mutu...   \n",
            "3  [consistency, training, has, proven, to, be, a...   \n",
            "4  [to, ensure, safety, in, automated, driving, t...   \n",
            "\n",
            "                              summaries_no_stopwords  \n",
            "0  [stereo, matching, widely, techniques, inferri...  \n",
            "1  [recent, advancements, artificial, intelligenc...  \n",
            "2  [paper, proposed, novel, mutual, consistency, ...  \n",
            "3  [consistency, training, proven, advanced, semi...  \n",
            "4  [ensure, safety, automated, driving, correct, ...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lemmatization**"
      ],
      "metadata": {
        "id": "8qSlhFpu2_Hw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatize_text_spacy(tokens):\n",
        "    doc = nlp(' '.join(tokens))\n",
        "    return [token.lemma_ for token in doc]\n",
        "\n",
        "df['lemmatized_summaries'] = df['summaries_no_stopwords'].apply(lemmatize_text_spacy)\n",
        "\n",
        "print(df[['summaries_no_stopwords', 'lemmatized_summaries']].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "omIXtyX63C-Y",
        "outputId": "c58a2d9c-df85-4a25-ffef-0d194e168304"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                              summaries_no_stopwords  \\\n",
            "0  [stereo, matching, widely, techniques, inferri...   \n",
            "1  [recent, advancements, artificial, intelligenc...   \n",
            "2  [paper, proposed, novel, mutual, consistency, ...   \n",
            "3  [consistency, training, proven, advanced, semi...   \n",
            "4  [ensure, safety, automated, driving, correct, ...   \n",
            "\n",
            "                                lemmatized_summaries  \n",
            "0  [stereo, matching, widely, technique, infer, d...  \n",
            "1  [recent, advancement, artificial, intelligence...  \n",
            "2  [paper, propose, novel, mutual, consistency, n...  \n",
            "3  [consistency, training, prove, advanced, semis...  \n",
            "4  [ensure, safety, automate, drive, correct, per...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Processed Abstracts**"
      ],
      "metadata": {
        "id": "HwzCiauA3oIQ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b79d1ff1",
        "outputId": "44a1ff42-8724-4286-dfb2-136abfd80ed9"
      },
      "source": [
        "def rejoin_summaries(lemmas):\n",
        "    return ' '.join(lemmas)\n",
        "\n",
        "df['final_summaries'] = df['lemmatized_summaries'].apply(rejoin_summaries)\n",
        "\n",
        "print(df[['lemmatized_summaries', 'final_summaries']].head())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                lemmatized_summaries  \\\n",
            "0  [stereo, matching, widely, technique, infer, d...   \n",
            "1  [recent, advancement, artificial, intelligence...   \n",
            "2  [paper, propose, novel, mutual, consistency, n...   \n",
            "3  [consistency, training, prove, advanced, semis...   \n",
            "4  [ensure, safety, automate, drive, correct, per...   \n",
            "\n",
            "                                     final_summaries  \n",
            "0  stereo matching widely technique infer depth s...  \n",
            "1  recent advancement artificial intelligence ai ...  \n",
            "2  paper propose novel mutual consistency network...  \n",
            "3  consistency training prove advanced semisuperv...  \n",
            "4  ensure safety automate drive correct perceptio...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qH_n9ufD3tDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62ef2d94"
      },
      "source": [
        "# Task\n",
        "Extract and display the most frequent noun phrases from the 'final_summaries' column of the dataframe."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fb965505"
      },
      "source": [
        "## Identify Noun Phrases\n",
        "\n",
        "### Subtask:\n",
        "Extract noun phrases from the 'final_summaries' column using spaCy's noun_chunks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14e6093c"
      },
      "source": [
        "**Reasoning**:\n",
        "To extract noun phrases, I will define a function that utilizes the pre-loaded spaCy 'nlp' object to process text and extract lemmatized noun chunks. This function will then be applied to the 'final_summaries' column to create a new 'noun_phrases' column.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fc0b8a23",
        "outputId": "171645fa-2e8e-4745-9df1-dd19f4c9219d"
      },
      "source": [
        "def extract_noun_phrases(text):\n",
        "    doc = nlp(text)\n",
        "    return [chunk.lemma_ for chunk in doc.noun_chunks]\n",
        "\n",
        "df['noun_phrases'] = df['final_summaries'].apply(extract_noun_phrases)\n",
        "\n",
        "print(df[['final_summaries', 'noun_phrases']].head())"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                     final_summaries  \\\n",
            "0  stereo matching widely technique infer depth s...   \n",
            "1  recent advancement artificial intelligence ai ...   \n",
            "2  paper propose novel mutual consistency network...   \n",
            "3  consistency training prove advanced semisuperv...   \n",
            "4  ensure safety automate drive correct perceptio...   \n",
            "\n",
            "                                        noun_phrases  \n",
            "0  [stereo matching, widely technique infer depth...  \n",
            "1  [recent advancement artificial intelligence, e...  \n",
            "2  [paper, novel mutual consistency network mcnet...  \n",
            "3  [consistency training, advanced semisupervise ...  \n",
            "4  [safety automate drive correct perception situ...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2299b29b"
      },
      "source": [
        "## Count Noun Phrase Frequencies\n",
        "\n",
        "### Subtask:\n",
        "Count the occurrences of each unique noun phrase identified.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08ad5d90"
      },
      "source": [
        "**Reasoning**:\n",
        "To count the occurrences of each unique noun phrase, I need to import the `Counter` class, flatten the list of lists in the 'noun_phrases' column into a single list, and then use `Counter` to get the frequencies.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03232298",
        "outputId": "5c1eee8d-b101-400a-e457-35371dea9b16"
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "all_noun_phrases = []\n",
        "for phrases_list in df['noun_phrases']:\n",
        "    all_noun_phrases.extend(phrases_list)\n",
        "\n",
        "noun_phrase_counts = Counter(all_noun_phrases)\n",
        "\n",
        "print(\"Top 10 most frequent noun phrases:\")\n",
        "for noun_phrase, count in noun_phrase_counts.most_common(10):\n",
        "    print(f\"'{noun_phrase}': {count}\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 most frequent noun phrases:\n",
            "'method': 55\n",
            "'algorithm': 51\n",
            "'deep learning': 38\n",
            "'paper': 37\n",
            "'propose method': 37\n",
            "'deep neural network': 30\n",
            "'image segmentation': 30\n",
            "'image': 27\n",
            "'approach': 26\n",
            "'performance': 25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2d876971"
      },
      "source": [
        "## Display Frequent Noun Phrases\n",
        "\n",
        "### Subtask:\n",
        "Present the most frequent noun phrases to the user.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cb4ccd7"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   Noun phrases were successfully extracted and lemmatized from the 'final\\_summaries' column, stored in a new column named 'noun\\_phrases'. For instance, a summary like \"stereo matching widely technique infer depth s...\" yielded noun phrases such as \"stereo matching\" and \"technique infer depth\".\n",
        "*   The frequency of each extracted noun phrase was calculated, and the top 10 most common phrases were identified.\n",
        "*   The most frequent noun phrases observed were:\n",
        "    *   'method': 55 occurrences\n",
        "    *   'algorithm': 51 occurrences\n",
        "    *   'deep learning': 38 occurrences\n",
        "    *   'paper': 37 occurrences\n",
        "    *   'propose method': 37 occurrences\n",
        "    *   'deep neural network': 30 occurrences\n",
        "    *   'image segmentation': 30 occurrences\n",
        "    *   'image': 27 occurrences\n",
        "    *   'approach': 26 occurrences\n",
        "    *   'performance': 25 occurrences\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The high frequency of terms like 'method', 'algorithm', 'deep learning', 'deep neural network', and 'image segmentation' suggests that the 'final\\_summaries' predominantly discuss research or technical papers related to artificial intelligence, machine learning, and computer vision.\n",
        "*   Further analysis could involve exploring the context in which these frequent noun phrases appear or performing topic modeling to group summaries based on shared themes indicated by these phrases.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9286a47"
      },
      "source": [
        "# Task\n",
        "Extract named entities from the 'final_summaries' column using spaCy's Named Entity Recognition (NER) capabilities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33cd5bbf"
      },
      "source": [
        "## Extract Named Entities\n",
        "\n",
        "### Subtask:\n",
        "Extract named entities from the 'final_summaries' column using spaCy's Named Entity Recognition (NER) capabilities.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25abdfee"
      },
      "source": [
        "**Reasoning**:\n",
        "To extract named entities, I will define a function that utilizes the pre-loaded spaCy 'nlp' object to process text and extract named entities, then apply this function to the 'final_summaries' column to create a new 'named_entities' column, and finally display the relevant columns to verify the extraction.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9b9ebaf7",
        "outputId": "4363c561-8ef8-4bb1-e3c5-5c501968908d"
      },
      "source": [
        "def extract_named_entities(text):\n",
        "    doc = nlp(text)\n",
        "    return [ent.text for ent in doc.ents]\n",
        "\n",
        "df['named_entities'] = df['final_summaries'].apply(extract_named_entities)\n",
        "\n",
        "print(df[['final_summaries', 'named_entities']].head())"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                     final_summaries          named_entities\n",
            "0  stereo matching widely technique infer depth s...                    [3d]\n",
            "1  recent advancement artificial intelligence ai ...              [european]\n",
            "2  paper propose novel mutual consistency network...  [misclassifie, second]\n",
            "3  consistency training prove advanced semisuperv...                      []\n",
            "4  ensure safety automate drive correct perceptio...      [recent year, gmm]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1b89429"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous step successfully extracted named entities. Now, I need to count the occurrences of each unique named entity to identify the most frequent ones.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cc5ec8b",
        "outputId": "a4e6f685-e494-4123-93f0-dfe64a0cfd62"
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "all_named_entities = []\n",
        "for entities_list in df['named_entities']:\n",
        "    all_named_entities.extend(entities_list)\n",
        "\n",
        "named_entity_counts = Counter(all_named_entities)\n",
        "\n",
        "print(\"Top 10 most frequent named entities:\")\n",
        "for entity, count in named_entity_counts.most_common(10):\n",
        "    print(f\"'{entity}': {count}\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 most frequent named entities:\n",
            "'3d': 304\n",
            "'cnn': 206\n",
            "'2d': 104\n",
            "'second': 78\n",
            "'1': 77\n",
            "'2': 77\n",
            "'3': 51\n",
            "'recent year': 40\n",
            "'scan': 31\n",
            "'one': 30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f16a85f2"
      },
      "source": [
        "## Display Frequent Named Entities\n",
        "\n",
        "### Subtask:\n",
        "Present the most frequent named entities and their counts to the user.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1422b3d"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   Named entities were successfully extracted from the `final_summaries` column, populating a new column named `named_entities` with lists of identified entities.\n",
        "*   The most frequent named entity identified was '3d', appearing 304 times.\n",
        "*   'cnn' was the second most frequent entity with 206 occurrences.\n",
        "*   '2d' ranked as the third most frequent entity, observed 104 times.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The prevalence of terms like '3d' and '2d' suggests that the content of the summaries frequently discusses dimensions, graphics, or visual technologies, while 'cnn' indicates a strong presence of news-related or media-specific topics.\n",
        "*   Future analysis could involve categorizing these frequent entities (e.g., by type like organizations, technologies, locations) to understand the dominant categories within the summaries, or exploring the context in which these entities appear to uncover specific trends or relationships.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6a27f55"
      },
      "source": [
        "# Task\n",
        "Define spaCy Matcher patterns to identify technical terms in the 'final_summaries' column, apply these patterns to extract the terms, and then analyze and display the most common extracted terms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1b1d04a5"
      },
      "source": [
        "## Define spaCy Matcher patterns\n",
        "\n",
        "### Subtask:\n",
        "Set up spaCy's Matcher and define linguistic patterns to identify technical terms. This may involve using Part-of-Speech (POS) tags, lemmas, or specific token attributes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d84b3a58"
      },
      "source": [
        "**Reasoning**:\n",
        "To set up spaCy's Matcher, I will import the `Matcher` class, initialize it with the `nlp.vocab`, define a few example patterns for technical terms, and then add these patterns to the matcher as requested by the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "591b1d27",
        "outputId": "1d5779ba-8b27-4e9b-c64a-e35e072d937c"
      },
      "source": [
        "from spacy.matcher import Matcher\n",
        "\n",
        "# Initialize the Matcher with the shared vocabulary\n",
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "# Define patterns for technical terms\n",
        "pattern1 = [{'LOWER': 'deep'}, {'LOWER': 'learning'}]\n",
        "pattern2 = [{'LOWER': 'neural'}, {'LOWER': 'network'}]\n",
        "pattern3 = [{'LOWER': 'computer'}, {'LOWER': 'vision'}]\n",
        "pattern4 = [{'POS': 'ADJ', 'OP': '?'}, {'POS': 'NOUN'}, {'POS': 'NOUN', 'OP': '?'}]\n",
        "\n",
        "# Add patterns to the matcher\n",
        "matcher.add('TECHNICAL_TERM', [pattern1, pattern2, pattern3, pattern4])\n",
        "\n",
        "print(\"spaCy Matcher initialized and patterns added.\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "spaCy Matcher initialized and patterns added.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59222573"
      },
      "source": [
        "## Apply Matcher to text\n",
        "\n",
        "### Subtask:\n",
        "Apply the defined patterns using the spaCy Matcher to the 'final_summaries' column to extract technical terms.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5631916b"
      },
      "source": [
        "**Reasoning**:\n",
        "To apply the defined spaCy Matcher patterns, I will create a function that processes text using the 'nlp' object, applies the 'matcher' to find all technical term matches, and returns a list of these extracted terms. This function will then be applied to the 'final_summaries' column to populate a new 'technical_terms' column.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e085e727",
        "outputId": "b18a0d85-24a1-421e-8c71-3c618fd8a064"
      },
      "source": [
        "def extract_technical_terms(text):\n",
        "    doc = nlp(text)\n",
        "    matches = matcher(doc)\n",
        "    extracted_terms = []\n",
        "    for match_id, start, end in matches:\n",
        "        extracted_terms.append(doc[start:end].text)\n",
        "    return extracted_terms\n",
        "\n",
        "df['technical_terms'] = df['final_summaries'].apply(extract_technical_terms)\n",
        "\n",
        "print(df[['final_summaries', 'technical_terms']].head())"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                     final_summaries  \\\n",
            "0  stereo matching widely technique infer depth s...   \n",
            "1  recent advancement artificial intelligence ai ...   \n",
            "2  paper propose novel mutual consistency network...   \n",
            "3  consistency training prove advanced semisuperv...   \n",
            "4  ensure safety automate drive correct perceptio...   \n",
            "\n",
            "                                     technical_terms  \n",
            "0  [matching, technique, technique infer, infer, ...  \n",
            "1  [recent advancement, recent advancement artifi...  \n",
            "2  [paper, mutual consistency, mutual consistency...  \n",
            "3  [consistency, consistency training, training, ...  \n",
            "4  [safety, automate drive, drive, correct percep...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dc8481b"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the technical terms have been extracted, the next step is to count the occurrences of each unique term. This will involve flattening the list of lists in the 'technical_terms' column and then using `collections.Counter` to get the frequencies. Finally, the top most frequent terms will be displayed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ce81a9fa",
        "outputId": "73c0132a-64fe-4c3c-bc27-b82f57d9159f"
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "all_technical_terms = []\n",
        "for terms_list in df['technical_terms']:\n",
        "    all_technical_terms.extend(terms_list)\n",
        "\n",
        "technical_term_counts = Counter(all_technical_terms)\n",
        "\n",
        "print(\"Top 10 most frequent technical terms:\")\n",
        "for term, count in technical_term_counts.most_common(10):\n",
        "    print(f\"'{term}': {count}\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 most frequent technical terms:\n",
            "'segmentation': 3328\n",
            "'image': 3255\n",
            "'method': 1383\n",
            "'network': 1288\n",
            "'image segmentation': 1209\n",
            "'model': 1113\n",
            "'approach': 680\n",
            "'performance': 658\n",
            "'task': 639\n",
            "'result': 620\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14ed85dc"
      },
      "source": [
        "## Analyze and display matched terms\n",
        "\n",
        "### Subtask:\n",
        "Present the most frequent technical terms and their counts to the user, and summarize the findings.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "de54ccf6"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The spaCy `Matcher` was successfully initialized with four patterns: three specific multi-word terms (`deep learning`, `neural network`, `computer vision`) and one general Part-of-Speech-based pattern that identifies noun phrases (allowing for an optional adjective and an optional second noun).\n",
        "*   A new column named `technical_terms` was added to the DataFrame, containing lists of extracted technical phrases from the `final_summaries` column for each entry.\n",
        "*   Analysis of the extracted terms revealed that 'segmentation' was the most frequent technical term, appearing 3328 times.\n",
        "*   Other highly frequent terms included 'image' with 3255 occurrences and 'method' with 1383 occurrences.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The current patterns successfully extract both specific multi-word terms and general noun phrases, providing a foundational set of technical terms. Further refinement of the general patterns could help focus on more domain-specific technical jargon, potentially by filtering out very common words like 'method' if they are not considered core technical terms for the specific domain.\n",
        "*   The extracted technical terms can serve as a valuable input for subsequent analyses, such as building a domain-specific lexicon, performing topic modeling, or generating keyword clouds to understand the predominant themes within the summaries.\n"
      ]
    }
  ]
}